{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# In script\n",
    "# from mtl import data_setup, engine, models, utils\n",
    "\n",
    "os.makedirs(os.getcwd() +'/mtl', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/plotting.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/plotting.py\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from mtl import utils\n",
    "\n",
    "# def plot_loss_curves_wrapper(directory: str, \n",
    "#                         mode: str, \n",
    "#                         y_lim=[-0.1, 10.0])-> None:\n",
    "#     from mtl import utils\n",
    "    \n",
    "#     files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "#     betas = [files[i].replace('.csv', '') for i in range(len(files))]\n",
    "    \n",
    "#     # For plotting\n",
    "#     for i in range(len(files)):\n",
    "#         history = {}\n",
    "#         df = pd.read_csv(directory + f'{files[i]}')\n",
    "#         history = df.to_dict(orient='list')\n",
    "#         history.pop('', None)\n",
    "#         try:\n",
    "#             history.pop('Unnamed: 0')\n",
    "#         except: pass\n",
    "#         beta = betas[i]\n",
    "#         utils.plot_loss_curves(history, y_lim=[y_lim[0], y_lim[1]], mode=mode, beta=beta)\n",
    "#     return None\n",
    "\n",
    "def plot_loss_curves(results: Dict[str, List[float]],\n",
    "                     y_lim = [-0.1, 10.],\n",
    "                     beta=0., \n",
    "                     mode='test'):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "        beta : current beta value for plot title\n",
    "        y_lim : Upper limit for plots\n",
    "        mode : 'test_only' or 'all'\n",
    "    \n",
    "    \n",
    "    Usage:     \n",
    "        directory = os.getcwd() + os.path.join('/history/', timestamp, experiment_name, extra)\n",
    "        \n",
    "        files = [f for f in os.listdir(directory) if os.path.isfile(directory + f)]\n",
    "        betas = [files[i].replace('.csv', '') for i in range(len(files))]\n",
    "        \n",
    "        # For plotting\n",
    "        for i in range(len(files)):\n",
    "            history = {}\n",
    "            df = pd.read_csv(directory + f'{files[i]}')\n",
    "            history = df.to_dict(orient='list')\n",
    "            history.pop('', None)\n",
    "            history.pop('Unnamed: 0')\n",
    "            beta = betas[i]\n",
    "            \n",
    "            utils.plot_loss_curves(history, y_lim=0.4, mode='test', beta=beta)      \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    if mode == 'all':\n",
    "        # Get the loss values of the results dictionary (training and test)\n",
    "        train_loss_main = results['train_loss_main']\n",
    "        train_loss_tot = results['train_loss_tot']\n",
    "        train_loss_aux = results['train_loss_aux']\n",
    "        test_loss_main = results['test_loss_main']\n",
    "\n",
    "        # Figure out how many epochs there were\n",
    "        epochs = range(len(results['train_loss_main']))\n",
    "\n",
    "        # Setup a plot \n",
    "        plt.figure(figsize=(20, 8))\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_loss_main, label='train_loss_main')\n",
    "        plt.plot(epochs, train_loss_aux, label='train_loss_aux')\n",
    "        plt.plot(epochs, train_loss_tot, label='train_loss_tot')\n",
    "        plt.ylim(y_lim[0], y_lim[1])\n",
    "        plt.xticks(np.arange(0, len(epochs), 10))\n",
    "        #plt.yticks(np.linspace(min(train_loss_tot), max(train_loss_tot)) )\n",
    "        #plt.tick_params(left = False, labelleft = False) \n",
    "        \n",
    "        #plt.title('Train Losses, Beta = ' + beta)\n",
    "        plt.title(f'Train Losses {beta}')\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plots main loss on train and test\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_loss_main, label='train_loss_main')\n",
    "        plt.plot(epochs, test_loss_main, label='test_loss_main')\n",
    "        plt.xticks(np.arange(0, len(epochs), 10))\n",
    "        plt.ylim(y_lim[0], y_lim[1])\n",
    "        #plt.tick_params(left = False, labelleft = False) \n",
    "        \n",
    "        plt.title(f'Main Losses, {beta}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "    elif mode=='test':\n",
    "        # Get the loss values of the results dictionary (training and test)\n",
    "        train_loss_main = results['train_loss_main']\n",
    "        test_loss_main = results['test_loss_main']\n",
    "\n",
    "        # Figure out how many epochs there were\n",
    "        epochs = range(len(results['train_loss_main']))\n",
    "\n",
    "        # Setup a plot \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        \n",
    "        # Plots main loss on train and test\n",
    "        plt.plot(epochs, train_loss_main, label='train_loss_main')\n",
    "        plt.plot(epochs, test_loss_main, label='test_loss_main')\n",
    "        plt.ylim(y_lim[0], y_lim[1])\n",
    "        #plt.tick_params(left = False, labelleft = False) \n",
    "        plt.xticks(np.arange(0, len(epochs), 10))\n",
    "\n",
    "        plt.title(f'Main Losses, {beta}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        \n",
    "# def plot_adv_curves(adv_dict: Dict[str, List[float]], mode='adv', \n",
    "#                     x_lim = [0., 200.], y_lim = [-0.1, 10.], save=False, save_name=None):\n",
    "#     ''' \n",
    "#     Args:\n",
    "#         * Path to history folder\n",
    "#         * mode: 'adv' for inverse advantage, \n",
    "#                 'std' for Test_loss(beta), \n",
    "#                 'vs' for L_alpha vs L_0\n",
    "#             gets called in utils.adv_from_csv(history_path, mode=mode)\n",
    "#         * y_lim: upper bound for plot\n",
    "#     Returns: None\n",
    "#     Plots: Advantage score curves vs Epochs for different betas\n",
    "#     '''\n",
    "\n",
    "#     import numpy as np\n",
    "#     import os\n",
    "#     from mtl_modular import utils\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import os\n",
    "#     import pandas as pd\n",
    "#     import warnings\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "#     from datetime import datetime\n",
    "#     import os\n",
    "#     from pathlib import Path\n",
    "#     from mtl_modular import utils\n",
    "\n",
    "#     epochs = np.arange(0., 500, 1)\n",
    "\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.title(f'Learning Curves')\n",
    "#     plt.xticks(np.arange(0, len(epochs), len(epochs)//10))\n",
    "#     plt.xlim(x_lim[0], x_lim[1])\n",
    "#     plt.ylim(y_lim[0], y_lim[1])\n",
    "#     plt.grid()\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Test Loss')\n",
    "    \n",
    "#     for key in adv_dict.keys():\n",
    "#         ll = len(adv_dict[key])\n",
    "#         plt.plot(epochs[:ll], adv_dict[key], label= f'{key}')\n",
    "#         plt.legend()\n",
    "        \n",
    "#     if save == True:\n",
    "#         if save_name is not None:\n",
    "#             plt.savefig(f'{history_path}{mode}_{save_name}.png', bbox_inches='tight')\n",
    "#         else:\n",
    "#             plt.savefig(f'{history_path}{mode}.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "def plot_adv_beta(adv_dicts, labels,\n",
    "                  y_lim=None, plot=True, save=False,\n",
    "                  directory=None, save_name=None, mode='adv', ylabel=None):\n",
    "\n",
    "    ''' \n",
    "    Args: \n",
    "        - adv_dicts : List of Dictionaries out of adv_from_csv function\n",
    "        - y_lim = None: upper y limit in plot\n",
    "        - plot = True : bool, whether to display plot\n",
    "        - save = False : bool, whether to save plot\n",
    "        - directory = None : str, where plot will be saved\n",
    "        - save_name = None : str, filename of png image\n",
    "        - mode = 'adv': 'adv' or 'loss' : label of y axis\n",
    "     \n",
    "    Returns: \n",
    "        - Dictionary: key = beta : value = advantage score\n",
    "        \n",
    "    Plots: \n",
    "        - Final Advantage score/Loss score vs Beta\n",
    "    '''\n",
    "#    CONFIG_PATH = os.getcwd() + '/config'\n",
    "#    config = load_config(path=CONFIG_PATH, config_name='1.yaml')\n",
    "#    N = config['N']\n",
    "#    DIM = config['DIM']\n",
    "#    hiddens = config['HIDDENS']\n",
    "#    title = f'Data Complexity: {len(hiddens)}, Data shape: [{N}, {DIM}], Random Seed: {seed} '\n",
    "\n",
    "    #plt.figure(figsize=(16, 8))\n",
    "    plt.grid()\n",
    "    \n",
    "    titles = ['Adv', 'Loss']\n",
    "    #colors = plt.cm.get_cmap('tab20')(np.linspace(0, 1, 20) )\n",
    "    \n",
    "    for i, adv_dict in list(enumerate(adv_dicts)):\n",
    "        best_list = [] ## Needs to be single value\n",
    "        betas = []\n",
    "        for beta in adv_dict.keys():\n",
    "            best_list.append(adv_dict[beta])\n",
    "            if beta not in betas:\n",
    "                betas.append(float(beta)) \n",
    "        \n",
    "        if plot:\n",
    "            plt.ylim(-0.0005, y_lim)\n",
    "            plt.plot(betas, best_list, label=labels[i])#, color=colors[i])\n",
    "            plt.scatter(x=betas, y=best_list) #, label=labels[i])\n",
    "            #print(f'betas = {betas}')\n",
    "            plt.xticks(np.linspace(0, betas[-1], num=10))\n",
    "            plt.title(f'{save_name}', fontsize=20)\n",
    "            plt.xlabel('Beta')\n",
    "\n",
    "            \n",
    "                \n",
    "            if mode=='adv':\n",
    "                plt.ylabel(ylabel)\n",
    "                plt.axhline(y=1., color='black', ls='--', lw=2.)\n",
    "            elif mode=='loss':\n",
    "                plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            #Da capire : manca history_path per settare titolo e cartella di destinazione\n",
    "            if save == True:\n",
    "                if save_name is not None:\n",
    "                    #plt.savefig(f'{history_path}{mode}_{save_name}.png', bbox_inches='tight')\n",
    "                    plt.savefig(f'{directory}_{save_name}.png', bbox_inches='tight')\n",
    "                else:\n",
    "                    #plt.savefig(f'{history_path}{mode}.png', bbox_inches='tight')\n",
    "                    print('Please, provide .png filename')\n",
    "            else: pass\n",
    "        \n",
    "    out = dict((key, value) for key, value in zip(betas, best_list))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curves(*curves, ylim1=-0.1, ylim2=3.):\n",
    "    '''\n",
    "    Use [] in place of missing lists to ignore\n",
    "    '''\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    for i, curve in list(enumerate(curves)):\n",
    "        plt.plot(curve, label=str(i+1))\n",
    "        \n",
    "    plt.ylim(ylim1, ylim2)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show\n",
    "\n",
    "\n",
    "def boxplot(subs_directory: str, metric: str, early_stopping=False) -> pd.DataFrame:\n",
    "\n",
    "    '''\n",
    "    Capisce file giusto in base a metric\n",
    "    '''\n",
    "\n",
    "    if metric not in ['MSE', '%Err', 'Pearson']:\n",
    "        raise Exception(\"metric should be one of 'MSE', '%Err', 'Pearson'\")\n",
    "\n",
    "    if early_stopping:\n",
    "        if metric=='MSE':\n",
    "            file_name = 'Loss_scores_early_stopping.csv'\n",
    "            y_label = r'$\\mathscr{L}$'\n",
    "        elif metric=='%Err':\n",
    "            file_name = '%Error_on_variance_early_stopping.csv'\n",
    "            y_label = '%Err on var(y)'\n",
    "        elif metric=='Pearson':\n",
    "            file_name = 'Pearson_early_stopping.csv'\n",
    "            y_label = r'$\\rho$'\n",
    "            \n",
    "    else:\n",
    "        if metric=='MSE':\n",
    "            file_name = 'Loss_scores_val_file.csv'\n",
    "            y_label = r'$\\mathscr{L}$'\n",
    "        elif metric=='%Err':\n",
    "            file_name = '%Error_on_variance_val_file.csv'\n",
    "            y_label = '%Err on var(y)'\n",
    "        elif metric=='Pearson':\n",
    "            file_name = 'Pearson_val_file.csv'\n",
    "            y_label = r'$\\rho$'\n",
    "\n",
    "    csv_file_path = subs_directory + file_name\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    keys = df.columns.tolist()\n",
    "\n",
    "    boxplot_y = [df[key].tolist() for key in keys]\n",
    "\n",
    "    means = [np.mean(boxplot_y[i]) for i in range(len(boxplot_y))]\n",
    "    std_devs = [np.std(boxplot_y[i]) for i in range(len(boxplot_y))]\n",
    "    \n",
    "    # Box plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(boxplot_y, labels=keys, patch_artist=True, boxprops=dict(facecolor='LightGreen'))\n",
    "    plt.axhline(means[0], color='red', ls='--', lw=.8)\n",
    "\n",
    "    # plt.yscale('symlog')\n",
    "\n",
    "    plt.xlabel(r'$\\beta$',  fontsize=16)\n",
    "    plt.ylabel(y_label,  fontsize=16)\n",
    "\n",
    "    # plt.axhline(y=0.953, color='red', ls='--', lw=1.7)\n",
    "    # plt.annotate('std', xy=(0, 0.953) , xytext=(-0.17, 0.953), annotation_clip=False, color='red')\n",
    "    # plt.xticks(np.linspace(0, keys[-1], num=10))\n",
    "\n",
    "    plt.title(f'{y_label} vs '+ r'$\\beta$')\n",
    "    plt.grid()\n",
    "    # plt.show()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def errorplot(subs_directory: str, \n",
    "                metric: str) -> pd.DataFrame:\n",
    "\n",
    "    if metric not in ['MSE', '%Err', 'Pearson']:\n",
    "        raise Exception(\"metric should be one of 'MSE', '%Err', 'Pearson'\")\n",
    "\n",
    "    if metric=='MSE':\n",
    "        file_name = 'Loss_scores_val_file.csv'\n",
    "        y_label = r'$\\mathscr{L}$'\n",
    "    elif metric=='%Err':\n",
    "        file_name = '%Error_on_variance_val_file.csv'\n",
    "        y_label = '%Err on var(y)'\n",
    "    elif metric=='Pearson':\n",
    "        file_name = 'Pearson_val_file.csv'\n",
    "        y_label = r'$\\rho$'\n",
    "\n",
    "    csv_file_path = subs_directory + file_name\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    keys = df.columns.tolist()\n",
    "\n",
    "    boxplot_y = [df[key].tolist() for key in keys]\n",
    "    \n",
    "    means = [np.mean(boxplot_y[i]) for i in range(len(boxplot_y))]\n",
    "    std_devs = [np.std(boxplot_y[i]) for i in range(len(boxplot_y))]\n",
    "    \n",
    "    # advvv = [means[i]/means[0] for i in range(len(means))]\n",
    "    # err_advvv = [(  np.sqrt( (np.square(means[i]*std_devs[0]) / (np.power(means[0], 4))) + np.square( std_devs[i] / means[0]  )  )  ) for i in range(len(means))]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Loss\n",
    "    plt.errorbar(keys, means, yerr=std_devs, fmt='o', capsize=5, markersize=3)\n",
    "    plt.axhline(y=means[0], color='red', ls='--', lw=1.)    \n",
    "\n",
    "    plt.xlabel(r'$\\beta$',  fontsize=18)\n",
    "    plt.ylabel(y_label,  fontsize=18)\n",
    "    plt.title(y_label + r'  vs  $\\beta$')  \n",
    "\n",
    "    # Adv\n",
    "    # plt.errorbar(keys, advvv, yerr=err_advvv, fmt='o', capsize=5, markersize=3)\n",
    "    # plt.axhline(y=1., color='red', ls='--', lw=1.)    \n",
    "    \n",
    "    # plt.xscale('symlog')\n",
    "    # plt.yscale('symlog')  \n",
    "    # plt.xticks([0, 0.2, 0.6, 1, 2, 3, 7, 10]) \n",
    "    # plt.yticks([0, 0.2, 0.5, 1, 2, 3, 4]) \n",
    "    # plt.ylim(ymin=0)\n",
    "    # plt.ylabel('$Adv$',  fontsize=18)\n",
    "    # plt.title('Adv vs Beta')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/utils.py\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "history_path = os.getcwd() + '/history/'\n",
    "\n",
    "def copy_file(yaml_file: str,\n",
    "                target_dir_path: str,\n",
    "              source=os.getcwd() + '/config/' ) -> None:\n",
    "    '''\n",
    "    Copies file from source to experiment folder specified by config file.\n",
    "    \n",
    "    Usage: copy_file(filename='my_config.yaml', \n",
    "              source=os.getcwd() + '/config/' )\n",
    "    \n",
    "    '''\n",
    "    # import utils\n",
    "    import shutil\n",
    "    # import os\n",
    "    # from datetime import datetime\n",
    "    \n",
    "    assert yaml_file.endswith(\".yaml\")\n",
    "    # config = utils.load_config(path=source, config_name=yaml_file)\n",
    "    \n",
    "    # experiments_path = os.getcwd() + '/experiments'\n",
    "    # timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "    # N = config['N']\n",
    "    # DIM = config['DIM']\n",
    "    # N_AUX = config['N_AUX']\n",
    "    # EPOCHS = config['EPOCHS']\n",
    "    # EXP = config['experiment_name']\n",
    "    # SUBEXP = config['subexperiment_name']\n",
    "    # directory = f'N={N}/DIM={DIM}/N_AUX={N_AUX}'\n",
    "\n",
    "    # directory = os.path.join(experiments_path, timestamp, EXP, directory, SUBEXP)\n",
    "    # target_dir_path = Path(directory)\n",
    "    # target_dir_path.mkdir(parents=True,\n",
    "    #                       exist_ok=True)\n",
    "    \n",
    "    shutil.copy(source + yaml_file, target_dir_path)\n",
    "    return None\n",
    "\n",
    "# Function to load yaml configuration file\n",
    "def load_config(path, config_name):\n",
    "    with open(os.path.join(path, config_name)) as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    return config\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "                target_dir_path: str,\n",
    "                 model_name: str):\n",
    "\n",
    "    \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "    Args:\n",
    "      model: A target PyTorch model to save.\n",
    "      target_dir: A directory for saving the model to.\n",
    "      model_name: A filename for the saved model. Should include\n",
    "        either \".pth\" or \".pt\" as the file extension.\n",
    "    \n",
    "    Example usage:\n",
    "      save_model(model=model_0,\n",
    "                 CONFIG_PATH = os.getcwd() + '/config',\n",
    "                 yaml_file='my_config.yaml'\n",
    "                 model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # from datetime import datetime\n",
    "    # import utils\n",
    "\n",
    "    # config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "    # experiments_path = os.getcwd() + '/experiments'\n",
    "    # timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "    # N = config['N']\n",
    "    # DIM = config['DIM']\n",
    "    # N_AUX = config['N_AUX']\n",
    "    # EPOCHS = config['EPOCHS']\n",
    "    # EXP = config['experiment_name']\n",
    "    # SUBEXP = config['subexperiment_name']\n",
    "    # directory = f'N={N}/DIM={DIM}/N_AUX={N_AUX}'\n",
    "\n",
    "    # directory = os.path.join(experiments_path, timestamp, EXP, directory, SUBEXP,)\n",
    "    # target_dir_path = Path(directory)\n",
    "    # target_dir_path.mkdir(parents=True,\n",
    "    #                       exist_ok=True)\n",
    "    \n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path} \\n\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "               f=model_save_path)\n",
    "\n",
    "\n",
    "def history_to_csv(history: dict, \n",
    "                    target_dir_path: str,\n",
    "                    model_name: str) -> None:\n",
    "    \n",
    "    \"\"\"Saves training history to a specific directory.\n",
    "    Input: \n",
    "            * experiment_name (same as in 'runs' directory)\n",
    "            * model_name may be some architecture characteristic (es. # auxiliary tasks or beta value) --> so far it's BETA\n",
    "            * extra may be additional info\n",
    "            \n",
    "    Usage:  utils.history_to_csv(history=history, \n",
    "                    target_dir_path = utils.path_handler(CONFIG_PATH = os.getcwd() + '/config', yaml_file='my_config',)\n",
    "                    model_name=f'Beta={BETAS[1]:.2f}')\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # from datetime import datetime\n",
    "    # import os\n",
    "    # from pathlib import Path\n",
    "    # import utils\n",
    "    \n",
    "    # # Config\n",
    "    # config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "\n",
    "    # # Create target directory\n",
    "    # experiments_path = os.getcwd() + '/experiments'\n",
    "    \n",
    "    # # Get timestamp of current date in reverse order\n",
    "    # timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "    # N = config['N']\n",
    "    # DIM = config['DIM']\n",
    "    # N_AUX = config['N_AUX']\n",
    "    # EPOCHS = config['EPOCHS']\n",
    "    # EXP = config['experiment_name']\n",
    "    # SUBEXP = config['subexperiment_name']\n",
    "    # directory = f'N={N}/DIM={DIM}/N_AUX={N_AUX}'\n",
    "\n",
    "    # directory = os.path.join(experiments_path, timestamp, EXP, directory, SUBEXP)\n",
    "\n",
    "    hist_df = pd.DataFrame(history)\n",
    "    hist_csv_file = target_dir_path / f'{model_name}.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f, index=False)\n",
    "            \n",
    "    print(f\"[INFO] Created .csv file to {target_dir_path}/{model_name}.csv\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def path_handler(CONFIG_PATH: str, yaml_file: str ) -> str:\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    # from mtl import utils\n",
    "    \n",
    "    def load_config(path, config_name):\n",
    "        with open(os.path.join(path, config_name)) as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        return config\n",
    "\n",
    "    # config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "    config = load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "\n",
    "    experiments_path = os.getcwd() + '/experiments'\n",
    "    timestamp = datetime.now().strftime(\"%m-%d-%Y\")\n",
    "\n",
    "    N = config['N']\n",
    "    DIM = config['DIM']\n",
    "    N_AUX = config['N_AUX']\n",
    "    EPOCHS = config['EPOCHS']\n",
    "    EXP = config['experiment_name']\n",
    "    SUBEXP = config['subexperiment_name']\n",
    "    ANTAGONIST = config['ANTAGONIST']\n",
    "    directory = f'N={N}/DIM={DIM}/N_AUX={N_AUX}/ANTAGONIST={ANTAGONIST}'\n",
    "\n",
    "    directory = os.path.join(experiments_path, timestamp, EXP, directory, SUBEXP)\n",
    "    target_dir_path = Path(directory)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                          exist_ok=True)\n",
    "    return target_dir_path\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "class Advantage:\n",
    "    '''\n",
    "    Usage: \n",
    "    import os \n",
    "    from mtl_modular import utils\n",
    "    directory = os.getcwd() + f'/experiments/07-02-2024/exp_113/sub_1/N=1000/DIM=20/N_AUX=1/EPOCHS=500/'\n",
    "    adv = Advantage() # Instantiate class\n",
    "    bb = adv.from_csv(path=directory) # Returns loss curves\n",
    "    best = adv.take_best(adv_dict=bb)\n",
    "    adv.compute_adv(best)\n",
    "    \n",
    "        OR\n",
    "        \n",
    "    import os \n",
    "    from mtl_modular import utils\n",
    "    directory = os.getcwd() + f'/experiments/07-02-2024/exp_113/sub_1/N=1000/DIM=20/N_AUX=1/EPOCHS=500/'\n",
    "    adv = utils.Advantage()\n",
    "    scores = adv.compute_adv(adv.take_best(adv_dict=adv.from_csv(path=directory)))\n",
    "    utils.plot_adv_beta(scores)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def from_csv(self, path=None, \n",
    "                mode='test'):\n",
    "        ''' \n",
    "        Arguments:\n",
    "        * PATH storing csv files. \n",
    "            directory = os.getcwd() + f'/experiments/05-02-2024/exp_5/sub_1/N=1000/DIM=20/N_AUX=1/EPOCHS=300/'\n",
    "            - mode: 'test': only keeps test loss\n",
    "                    \n",
    "                    'all': keeps all training history\n",
    "                \n",
    "        Returns: dictionary -> Beta : testing loss curve of training run\n",
    "                                lenght is equal to training EPOCHS\n",
    "    \n",
    "        May be used with utils.plot_adv_beta as:\n",
    "                        utils.plot_adv_beta(utils.from_csv(directory))\n",
    "        '''\n",
    "    \n",
    "\n",
    "        files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "        keys = [files[i].replace('.csv', '') for i in range(len(files))]\n",
    "        keys = [keys[i].replace('Beta=', '') for i in range(len(keys))]\n",
    "        values = []\n",
    "        columns = ['test0']\n",
    "        # if mode=='test':\n",
    "        #     columns = ['test_loss_main']\n",
    "        # elif mode=='all':\n",
    "        #     columns = ['train_loss_main','train_loss_tot','train_loss_aux','test_loss_main','test_loss_aux']\n",
    "        for i in range(len(files)):\n",
    "            df = pd.read_csv(path + f'{files[i]}', usecols=columns)\n",
    "            hhh = df.to_dict(orient='list')\n",
    "            # print(hhh)\n",
    "            # Only need test_loss\n",
    "            hhh_list = hhh['test0']\n",
    "            # print(hhh_list)\n",
    "            # Returns loss\n",
    "            values.append(hhh_list)\n",
    "        out_dict = dict((key, value) for key, value in zip(keys,values))\n",
    "        return out_dict   \n",
    "    \n",
    "\n",
    "    def take_best(self, adv_dict: Dict[str, List[float]]):\n",
    "        keys = list(adv_dict.keys())\n",
    "        values = [min(adv_dict[key]) for key in adv_dict.keys()]\n",
    "        return dict((key, value) for key, value in zip(keys, values))\n",
    "    \n",
    "    def take_last(self, adv_dict: Dict[str, List[float]]):\n",
    "        keys = list(adv_dict.keys())\n",
    "        values = [adv_dict[key][-1] for key in adv_dict.keys()]\n",
    "        return dict((key, value) for key, value in zip(keys, values))\n",
    "\n",
    "    def compute_adv(self, adv_dict: Dict[str, List[float]]):\n",
    "        keys = list(adv_dict.keys())\n",
    "        #print(adv_dict[keys[0]])\n",
    "        try:\n",
    "            L0 = adv_dict['0.00']\n",
    "        except: \n",
    "            print('L0 cannot be retrieved from file')\n",
    "        values = [adv_dict[key]/L0 for key in adv_dict.keys()]\n",
    "        return dict((key, value) for key, value in zip(keys, values))\n",
    "    \n",
    "    \n",
    "    def runs_to_dict(self, subs_directory):\n",
    "        '''\n",
    "        Retrieves best score for loss and computes advantage score for each run and returns full dictionary\n",
    "        Args:\n",
    "         - subs_directory : experiment folder path -> subs should be equivalent runs when using this function.\n",
    "        Returns:\n",
    "         - Dictionary -> result_dict, loss_dict\n",
    "                     { BETA: [adv_run1, adv_run2, ..] },  { BETA: [loss_run1, loss_run2, ..] }\n",
    "        Usage: \n",
    "        \n",
    "                import os\n",
    "                from mtl_modular import utils\n",
    "                \n",
    "                path = os.getcwd() + f'/experiments/08-02-2024/exp_115/'\n",
    "                adv = utils.Advantage()\n",
    "                rr = adv.runs_to_dict(path)\n",
    "            \n",
    "                #To get mean and std, use:\n",
    "                compute_stats(rr)\n",
    "        '''\n",
    "        adv_list = []\n",
    "        loss_list = []\n",
    "    \n",
    "        for dirpath, dirnames, filenames in os.walk(subs_directory):\n",
    "            if not dirnames:\n",
    "                losses = self.take_last(adv_dict=self.from_csv(path=dirpath + '/'))\n",
    "                loss_list.append(losses)\n",
    "                scores = self.compute_adv(losses)\n",
    "                adv_list.append(scores)\n",
    "        \n",
    "        adv_dict = {}\n",
    "        loss_dict = {}\n",
    "        \n",
    "        for my_dict in adv_list:\n",
    "            for key, value in my_dict.items():\n",
    "                if key in adv_dict:\n",
    "                    adv_dict[key].append(value)\n",
    "                else:\n",
    "                    adv_dict[key] = [value]\n",
    "                    \n",
    "        for my_dict in loss_list:\n",
    "            for key, value in my_dict.items():\n",
    "                if key in loss_dict:\n",
    "                    loss_dict[key].append(value)\n",
    "                else:\n",
    "                    loss_dict[key] = [value]\n",
    "\n",
    "        return adv_dict, loss_dict\n",
    "\n",
    "    def compute_stats(self, result_dict):    \n",
    "        '''\n",
    "        Compute mean and standard deviation for each key, where values are lists.\n",
    "        Args: \n",
    "            - Dictionary output of runs_to_dict function.\n",
    "        Returns:\n",
    "            - Dictionary -> { BETA: [mean, std_dev] }\n",
    "        Usage: \n",
    "                import os\n",
    "                from mtl_modular import utils\n",
    "                path = os.getcwd() + f'/experiments/08-02-2024/exp_115/'\n",
    "                adv = utils.Advantage()\n",
    "                rr = adv.runs_to_dict(path)\n",
    "            #To get mean and std, use:\n",
    "                compute_stats(rr)\n",
    "            \n",
    "        '''\n",
    "        # Compute mean and standard deviation for each key\n",
    "        result_stats = {}\n",
    "        for key, values in result_dict.items():\n",
    "            mean_value = np.mean(values)\n",
    "            std_deviation = np.std(values)\n",
    "            # std_deviation = ( np.min(values) + np.max(values) )/2\n",
    "            result_stats[key] = {'mean': mean_value, 'std': std_deviation}\n",
    "        \n",
    "        #for key, values in result_stats.items():\n",
    "        #    print(key, values)\n",
    "        return result_stats\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "def get_data_distrib(subs_directory, plot=False):\n",
    "    import os\n",
    "    from mtl import utils\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    import torch\n",
    "    \n",
    "    #directory = os.getcwd() + f'/experiments/29-02-2024/exp_prova/sub_9/N=45/DIM=20/N_AUX=1/EPOCHS=500/'\n",
    "    #directory = os.getcwd() + f'/experiments/03-03-2024/exp_big_student/sub_1/N=10/DIM=20/N_AUX=1/EPOCHS=500/'\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(subs_directory):\n",
    "        # Check if the current directory has subdirectories\n",
    "        if not dirnames:\n",
    "            directory = dirpath\n",
    "    \n",
    "    yaml_file =  [f for f in os.listdir(directory) if f.endswith('.yaml')][0]\n",
    "    config = utils.load_config(path=directory, config_name=yaml_file)\n",
    "    \n",
    "    C = len(config['HIDDENS'])\n",
    "    S = len(config['S_HIDDENS'])\n",
    "    C_list = list(config['HIDDENS'])\n",
    "    S_list = list(config['S_HIDDENS'])\n",
    "\n",
    "    N = config['N']\n",
    "    DIM = config['DIM']\n",
    "    N_AUX = config['N_AUX']\n",
    "    \n",
    "    print(f'Complexity: {C} {C_list} | Student: {S} {S_list}')\n",
    "\n",
    "    all_data_path = os.getcwd() + '/data/'\n",
    "    sub_path = f'DIM={DIM}/N_AUX={N_AUX}/C={C}/'\n",
    "    data_dir = os.path.join(all_data_path, sub_path)\n",
    "    #data_dir = os.getcwd() + f'/data/N={N}/DIM=20/N_AUX=1/C={C}/'\n",
    "    \n",
    "    print(f'Loading test data from :\\n {data_dir}')\n",
    "    train_data = torch.load(data_dir + 'test_data.pt')\n",
    "    \n",
    "    labels = []\n",
    "    for X, y in train_data:\n",
    "        labels.append(float(y[0].detach()))\n",
    "    \n",
    "    mean = np.mean(labels)\n",
    "    std = np.std(labels)\n",
    "    var = np.var(labels)\n",
    "    if plot:\n",
    "        plt.hist(labels, bins=100)\n",
    "        plt.title(f'Mean = {mean:.3f} | Var = {var:.3f}')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    else: pass\n",
    "    print(f'Mean = {mean:.3f} | Var = {var:.3f}')\n",
    "    return mean, var\n",
    "    \n",
    "### Usage:\n",
    "# import os\n",
    "# subs_directory = os.getcwd() + f'/data/DIM=20/N_AUX=1/C=1/'\n",
    "# mean, var = get_data_distrib(subs_directory=subs_directory, plot=True)[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def exp_to_csv(subs_directory: str) -> None:\n",
    "    '''\n",
    "    Takes experiment path (with sub1, sub2, ..), creates and saves a csv file\n",
    "    with rows = runs, columns = betas\n",
    "    each entry is the last LOSS SCORE\n",
    "    '''\n",
    "    list_of_lists = []\n",
    "\n",
    "    # Per ogni run\n",
    "    for dirpath, dirnames, filenames in os.walk(subs_directory):\n",
    "        if not dirnames:\n",
    "            files = [f for f in os.listdir(dirpath) if f.endswith('.csv')]\n",
    "            keys = [files[i].replace('.csv', '') for i in range(len(files))]\n",
    "            keys = [keys[i].replace('Beta=', '') for i in range(len(keys))]\n",
    "            keys = [float(key) for key in keys]\n",
    "\n",
    "            loss_list = []\n",
    "\n",
    "            # Per ogni beta\n",
    "            columns = ['test0']\n",
    "            for file in files:\n",
    "                df = pd.read_csv(dirpath + f'/{file}', usecols=columns)\n",
    "                l = df['test0'].iloc[-1]\n",
    "                loss_list.append(l)\n",
    "\n",
    "            # Ordering\n",
    "            keys, losses = (list(t) for t in zip(*sorted(zip(keys, loss_list))))\n",
    "\n",
    "            # list_of_lists[0] is the first run \n",
    "            list_of_lists.append(losses)\n",
    "\n",
    "    df = pd.DataFrame(list_of_lists)\n",
    "    df.columns = [str(key) for key in keys]\n",
    "    df.head()\n",
    "\n",
    "    # Loss_scores file\n",
    "    hist_csv_file = subs_directory + 'Loss_scores_early_stopping.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "\n",
    "    print(f\"[INFO] Created .csv file to {hist_csv_file}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# def PearsonCoeff(true_labels, predictions):\n",
    "#     import torch\n",
    "#     pearson = torch.corrcoef(torch.stack([true_labels, predictions], dim=0))[0, 1].item()\n",
    "#     return pearson\n",
    "\n",
    "# def Error_on_variance(true_labels, predictions):\n",
    "#     var = true_labels[:, 0].var().item()\n",
    "#     error = torch.nn.MSELoss(true_labels, predictions) / var\n",
    "#     return error\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "\n",
    "class PearsonCoeff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PearsonCoeff, self).__init__()\n",
    "\n",
    "    def forward(self, true_labels, predictions):\n",
    "        pearson = torch.corrcoef(torch.stack([true_labels, predictions], dim=0))[0, 1]\n",
    "        return pearson\n",
    "\n",
    "class ErrorOnVariance(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ErrorOnVariance, self).__init__()\n",
    "\n",
    "    def forward(self, true_labels, predictions):\n",
    "        var = true_labels.var().item()\n",
    "        mse_loss = torch.nn.MSELoss()\n",
    "        error = mse_loss(predictions, true_labels) / var\n",
    "        return error\n",
    "\n",
    "def advantage_error(means: List,\n",
    "                    std_devs: List) -> List:\n",
    "    '''\n",
    "    Input : means = list of mean losses score\n",
    "            std_devs = list of standard deviations for losses score\n",
    "    Returns: err = list of error on advantage score\n",
    "    Usage: adv_err = utils.advantage_error(means, std_devs)\n",
    "    '''\n",
    "    # err = [(  np.sqrt( (np.square(means[i]*std_devs[0]) / (np.power(means[0], 4))) + np.square( std_devs[i] / means[0]  )  )  ) for i in range(len(means))]\n",
    "    # err = [np.sqrt( np.square( std_devs[i] / means[0]  ) + np.square(\t(means[i]*std_devs[0]\t/ np.power(means[0], 2)\t)) for i in range(len(means))]\n",
    "    err = [(means[i]/means[0]) * np.sqrt(  np.square(std_devs[i] / means[i]) + np.square(std_devs[0] / means[0])    ) for i in range(len(means))]\n",
    "    return err\n",
    "\n",
    "\n",
    "def evaluate_model(data_path: str, \n",
    "                    subs_directory: str,\n",
    "                    yaml_file: str,\n",
    "                    metric: str\n",
    "                    ) -> List:\n",
    "\n",
    "    '''\n",
    "    Usage: \n",
    "        for N_AUX in range(1, 10):\n",
    "            data_path = os.getcwd() + f'/data/DIM=20/N_AUX={N_AUX}/C=1'\n",
    "            subs_directory = os.getcwd() + f'/experiments/04-20-2024/exp_aux{N_AUX}/'\n",
    "            yaml_file = f'exp_aux{N_AUX}/1.yaml'\n",
    "            utils.evaluate_model(data_path=data_path, subs_directory=subs_directory, yaml_file=yaml_file, boxplot=False)\n",
    "    Returns: \n",
    "        Pandas DataFrame, columns: beta | rows: single runs | entries: Loss score\n",
    "    '''\n",
    "    if metric not in ['MSE', '%Err', 'Pearson', 'RMSE']:\n",
    "        raise Exception(\"metric should be one of 'MSE', '%Err', 'Pearson', 'RMSE'\")\n",
    "\n",
    "    from mtl import models, plotting, utils\n",
    "    import os \n",
    "    device = 'cpu'\n",
    "    \n",
    "    torch.manual_seed(3)\n",
    "\n",
    "    # Data from file ----------------------------------\n",
    "    test_data = torch.load(data_path + '/val_data.pt')\n",
    "    X_test = test_data.tensors[0]\n",
    "    y_test = test_data.tensors[1]\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    student = models.Student(yaml_file=yaml_file)\n",
    "    if metric=='MSE':\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "    elif metric=='%Err':\n",
    "        loss_fn = utils.ErrorOnVariance()\n",
    "    elif metric=='Pearson':\n",
    "        loss_fn = utils.PearsonCoeff()\n",
    "    elif metric=='RMSE':\n",
    "        loss_fn = utils.RMSELoss()\n",
    "\n",
    "    list_of_lists = []\n",
    "    # Per ogni run\n",
    "    for dirpath, dirnames, filenames in os.walk(subs_directory):\n",
    "        if not dirnames:\n",
    "            files = [f for f in os.listdir(dirpath) if f.endswith('.pth')]\n",
    "            keys = [files[i].replace('.pth', '') for i in range(len(files))]\n",
    "            keys = [keys[i].replace('Beta=', '') for i in range(len(keys))]\n",
    "            keys = [float(key) for key in keys]\n",
    "            keys, files = (list(t) for t in zip(*sorted(zip(keys, files))))\n",
    "\n",
    "            # Per ogni beta\n",
    "            losses = []\n",
    "            for file in files:\n",
    "                run = dirpath + '/' + file\n",
    "                state_dict = torch.load(run,  map_location=torch.device('cpu'))\n",
    "                student.load_state_dict(state_dict)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    y_pred = student(X_test)\n",
    "                    l = loss_fn(y_pred[:, 0], y_test[:, 0])\n",
    "                    losses.append(l.item())\n",
    "            list_of_lists.append(losses)\n",
    "\n",
    "    df = pd.DataFrame(list_of_lists)\n",
    "    df.columns = [str(key) for key in keys]\n",
    "\n",
    "# csv file name\n",
    "    if metric=='MSE':\n",
    "        file_name = 'Loss_scores_val_file.csv'\n",
    "    elif metric=='%Err':\n",
    "        file_name = '%Error_on_variance_val_file.csv'\n",
    "    elif metric=='Pearson':\n",
    "        file_name = 'Pearson_val_file.csv'\n",
    "    elif metric=='RMSE':\n",
    "        file_name = 'RMSE_val_file.csv'\n",
    "\n",
    "\n",
    "    hist_csv_file = subs_directory + file_name\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print(f\"[INFO] Created .csv file to {hist_csv_file}\")\n",
    "\n",
    "    # out = [np.std(df[f'{key}']) for key in keys]\n",
    "    # return out # standard dev across runs for fixed beta\n",
    "    return df\n",
    "\n",
    "\n",
    "def adv_and_err_from_df(df, beta, metric):\n",
    "    '''\n",
    "    Computes advantage and its error bar from full result dataframe\n",
    "    Usage: df_adv, df_adv_err = utils.adv_and_err_from_df(df=df, beta=0.2)\n",
    "    '''\n",
    "    import pandas as pd\n",
    "\n",
    "    df_beta = df.loc[df['BETA']==beta]\n",
    "    df_0 = df.loc[df['BETA']==0.0]\n",
    "\n",
    "    df_nn = df_beta.pivot('N', 'N_AUX', metric)\n",
    "    df_nn0 = df_0.pivot(\"N\", \"N_AUX\" , metric)\n",
    "\n",
    "    df_nn_err = df_beta.pivot('N', 'N_AUX', 'STD_'+ metric)\n",
    "    df_nn0_err = df_0.pivot('N', 'N_AUX', 'STD_'+ metric)\n",
    "\n",
    "    df_adv = 1 - df_nn.div(df_nn0, axis=0)\n",
    "    df_adv_err = df_nn.div(df_nn0) * np.sqrt(  ( df_nn_err.div(df_nn) )**2 + ( df_nn0_err.div(df_nn0) )  **2 )\n",
    "\n",
    "    return df_adv, df_adv_err\n",
    "\n",
    "def score_and_err_from_df(df, beta, metric):\n",
    "    df_beta = df.loc[df['BETA']==beta]\n",
    "    df_nn = df_beta.pivot('N', 'N_AUX', metric)\n",
    "    df_nn_err = df_beta.pivot('N', 'N_AUX', 'STD_'+ metric)\n",
    "    return df_nn, df_nn_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_AUX = 3\n",
    "# yaml_file = 'exp_aux3/1.yaml'\n",
    "# N = 100\n",
    "# DIM = 20\n",
    "# teacher = models.Teacher(n_aux=N_AUX, yaml_file=yaml_file)#.to(device)\n",
    "# # torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "# # train_data, test_data = data_setup.make_data(Teacher=teacher, N=N, dim=DIM)\n",
    "# train, test = make_data(yaml_file=yaml_file)\n",
    "\n",
    "# # train.tensors[0].shape, train.tensors[1].shape, test.tensors[0].shape, test.tensors[1].shape\n",
    "# tt = test.tensors[1]\n",
    "# print(tt)\n",
    "# y_labels = tt[:, 0]\n",
    "# print(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# tt = test.tensors[1]\n",
    "# y_labels = tt[:, 0].numpy()\n",
    "# aux_labels = tt[:, 1].numpy()\n",
    "\n",
    "# # ------- Main ------------------------------\n",
    "# y_mean = np.mean(y_labels)\n",
    "# y_std = np.std(y_labels)\n",
    "# y_var = np.var(y_labels)\n",
    "\n",
    "# plt.hist(y_labels, bins=100)\n",
    "# plt.title(f'Main task label distribution y\\n\\nMean = {y_mean:.3f} | Var = {y_var:.3f}')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "# print(f'Mean = {y_mean:.3f} | Var = {y_var:.3f}')\n",
    "\n",
    "# # ------- Aux ------------------------------\n",
    "# aux_mean = np.mean(aux_labels)\n",
    "# aux_std = np.std(aux_labels)\n",
    "# aux_var = np.var(aux_labels)\n",
    "\n",
    "# plt.hist(aux_labels, bins=100)\n",
    "# alpha = \"\\u03B1\"\n",
    "# plt.title(f'Aux task label distribution {alpha}\\n\\nMean = {aux_mean:.3f} | Var = {aux_var:.3f}')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "# print(f'Mean = {aux_mean:.3f} | Var = {aux_var:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/data_setup.py\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "try :\n",
    "    import models, utils\n",
    "except:\n",
    "    from mtl import models, utils\n",
    "    \n",
    "def make_data(yaml_file: str):\n",
    "    \n",
    "    '''\n",
    "    Generates 10.000 data points with labels given by teacher, normalizes \n",
    "    '''\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Config\n",
    "    CONFIG_PATH = os.getcwd() + '/config'\n",
    "    config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "    DIM=config['DIM']\n",
    "    N_AUX = config['N_AUX']\n",
    "    experiment_name = config['experiment_name']\n",
    "\n",
    "    torch.manual_seed(3)\n",
    "\n",
    "    data_path = os.getcwd() + '/data/'\n",
    "    sub_path = f'DIM={DIM}/N_AUX={N_AUX}'\n",
    "    save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    save_path = str(save_path)\n",
    "\n",
    "    # Actions\n",
    "    X = torch.randn(104000, DIM, requires_grad=True).detach().to(device)\n",
    "    teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "    torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        y = teacher(X).to(device)\n",
    "\n",
    "    y_new = y.clone()\n",
    "    for task in range(0, N_AUX+1):\n",
    "      mean = torch.mean(y[:, task])\n",
    "      std = torch.std(y[:, task])\n",
    "      vals = (y[:, task] - mean) / std\n",
    "      y_new[:, task] = vals\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=2000, random_state=42, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2000, random_state=42, shuffle=True)\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    return train_data, test_data, val_data, save_path\n",
    "\n",
    "\n",
    "def make_data_noise(yaml_file: str):\n",
    "    \n",
    "    '''\n",
    "    Generates 10.000 data points with labels given by teacher, normalizes \n",
    "    '''\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Config\n",
    "    CONFIG_PATH = os.getcwd() + '/config'\n",
    "    config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "    DIM=config['DIM']\n",
    "    N_AUX = config['N_AUX']\n",
    "    NOISE = config['NOISE']\n",
    "    experiment_name = config['experiment_name']\n",
    "\n",
    "    torch.manual_seed(3)\n",
    "\n",
    "    data_path = os.getcwd() + '/data/'\n",
    "    sub_path = f'DIM={DIM}/N_AUX={N_AUX}/NOISE={NOISE}'\n",
    "    save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    save_path = str(save_path)\n",
    "\n",
    "    # Actions\n",
    "    X = torch.randn(10000, DIM, requires_grad=True).detach().to(device)\n",
    "    X_noise = X + torch.empty(10000, DIM).normal_(mean=0, std=NOISE) # vedi documentazione\n",
    "    teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "    torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      y = teacher(X).to(device)\n",
    "      y_noisy = teacher(X_noise).to(device)\n",
    "\n",
    "    yy = y[:, 0]\n",
    "    mean = torch.mean(yy)\n",
    "    std = torch.std(yy)\n",
    "    vals = (yy - mean) / std\n",
    "    yy = vals.unsqueeze(dim=1)\n",
    "\n",
    "    aa = y_noisy[:, 1]\n",
    "    mean = torch.mean(aa)\n",
    "    std = torch.std(aa)\n",
    "    vals = (aa - mean) / std\n",
    "    aa = vals.unsqueeze(dim=1)\n",
    "\n",
    "    y_new = torch.cat([yy, aa], dim=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=2000, random_state=42, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2000, random_state=42, shuffle=True)\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    return train_data, test_data, val_data, save_path\n",
    "\n",
    "\n",
    "def make_data_noisy_labels(yaml_file: str):\n",
    "    \n",
    "    '''\n",
    "    Generates 10.000 data points with labels given by teacher, normalizes \n",
    "    '''\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Config\n",
    "    CONFIG_PATH = os.getcwd() + '/config'\n",
    "    config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "    DIM=config['DIM']\n",
    "    N_AUX = config['N_AUX']\n",
    "    NOISE = config['NOISE']\n",
    "    experiment_name = config['experiment_name']\n",
    "\n",
    "    torch.manual_seed(3)\n",
    "\n",
    "    data_path = os.getcwd() + '/data/'\n",
    "    sub_path = f'DIM={DIM}/N_AUX={N_AUX}/NOISE={NOISE}'\n",
    "    save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    save_path = str(save_path)\n",
    "\n",
    "    # Actions\n",
    "    X = torch.randn(10000, DIM, requires_grad=True).detach().to(device)\n",
    "    teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "    torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        y = teacher(X).to(device)\n",
    "\n",
    "    y_new = y.clone()\n",
    "    for task in range(0, N_AUX+1):\n",
    "      mean = torch.mean(y[:, task])\n",
    "      std = torch.std(y[:, task])\n",
    "      vals = (y[:, task] - mean) / std\n",
    "      y_new[:, task] = vals\n",
    "      y_new[:, 1:] = y_new[:, 1:] + torch.empty(10000, 1).normal_(mean=0, std=NOISE)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=2000, random_state=42, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2000, random_state=42, shuffle=True)\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    return train_data, test_data, val_data, save_path\n",
    "\n",
    "\n",
    "\n",
    "def make_data_noisy_antagonist(yaml_file: str):\n",
    "    '''\n",
    "    Generates 10.000 data points with labels given by teacher, normalizes \n",
    "    '''\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Config\n",
    "    CONFIG_PATH = os.getcwd() + '/config'\n",
    "    config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "    DIM=config['DIM']\n",
    "    N_AUX = config['N_AUX']\n",
    "    NOISE = config['NOISE']\n",
    "    ANTAGONIST = config['ANTAGONIST']\n",
    "    experiment_name = config['experiment_name']\n",
    "\n",
    "    torch.manual_seed(3)\n",
    "\n",
    "    data_path = os.getcwd() + '/data/'\n",
    "    sub_path = f'DIM={DIM}/N_AUX={N_AUX}/ANTAGONIST={ANTAGONIST}'\n",
    "    save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    save_path = str(save_path)\n",
    "\n",
    "    # Actions\n",
    "    X = torch.randn(100000, DIM, requires_grad=True).detach().to(device)\n",
    "    teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "    torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        y = teacher(X).to(device)\n",
    "\n",
    "    y_new = y.clone()\n",
    "    for task in range(0, N_AUX+1):\n",
    "      mean = torch.mean(y[:, task])\n",
    "      std = torch.std(y[:, task])\n",
    "      vals = (y[:, task] - mean) / std\n",
    "      y_new[:, task] = vals\n",
    "      if ANTAGONIST == 0:\n",
    "        pass\n",
    "      else:\n",
    "        y_new[:, -ANTAGONIST:] = y_new[:, -ANTAGONIST:] + torch.empty(100000, 1).normal_(mean=0, std=NOISE)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size=2000, random_state=42, shuffle=True)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=2000, random_state=42, shuffle=True)\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    test_data = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    return train_data, test_data, val_data, save_path\n",
    "\n",
    "\n",
    "def make_dataloaders(\n",
    "    train_data: torch.tensor, \n",
    "    test_data: torch.tensor,\n",
    "    batch_size: int, \n",
    "    num_workers: int=0):\n",
    "    \n",
    "    \"\"\"Creates training and testing DataLoaders.\n",
    "    Args:\n",
    "      train_data, test_data: torch.tensors (output of make_data)\n",
    "      batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "      num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "    Returns:\n",
    "      A tuple of (train_dataloader, test_dataloader).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=int(0),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, # don't need to shuffle test data\n",
    "        num_workers=int(0),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## create_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/create_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/create_dataset.py\n",
    "\n",
    "'''\n",
    "# Command line arguments --> only config file (https://www.youtube.com/watch?v=FbEJN8FsJ9U)\n",
    "\n",
    "Usage !python mtl/create_dataset.py exp_1/1.yaml\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from mtl import data_setup, models, engine, utils\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Config\n",
    "parser = argparse.ArgumentParser(description='specifies config file')\n",
    "parser.add_argument('yaml_file', metavar='yaml_file', type=str, help='specify config file name with .yaml extension')\n",
    "args = parser.parse_args()\n",
    "yaml_file = args.yaml_file\n",
    "CONFIG_PATH = os.getcwd() + '/config'\n",
    "config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "N=config['N']\n",
    "DIM=config['DIM']\n",
    "N_AUX = config['N_AUX']\n",
    "COMPLEXITY = len(config['HIDDENS'])\n",
    "experiment_name = config['experiment_name']\n",
    "\n",
    "# Save path\n",
    "# timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "\n",
    "# data_path = os.getcwd() + '/data/'\n",
    "# sub_path = f'DIM={DIM}/N_AUX={N_AUX}'#/C={COMPLEXITY}'\n",
    "# save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "# save_path.mkdir(parents=True,\n",
    "#                 exist_ok=True)\n",
    "# save_path = str(save_path)\n",
    "\n",
    "# Actions\n",
    "teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "# torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "train_data, test_data, val_data, save_path = data_setup.make_data(yaml_file=yaml_file)\n",
    "\n",
    "torch.save(train_data, save_path + '/train_data.pt')\n",
    "torch.save(test_data, save_path + '/test_data.pt')\n",
    "torch.save(val_data, save_path + '/val_data.pt')\n",
    "\n",
    "print(f'Datafiles saved to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_dataset_noise.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/create_dataset_noise.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/create_dataset_noise.py\n",
    "\n",
    "'''\n",
    "# Command line arguments --> only config file (https://www.youtube.com/watch?v=FbEJN8FsJ9U)\n",
    "\n",
    "Usage:\n",
    "    import numpy as np\n",
    "    noise_list = ['0.10', '0.20', '0.30', '0.40', '0.50', '0.60', '0.70', '0.80', '0.90', '1.00', '1.10']\n",
    "    for NOISE in noise_list:\n",
    "        !python mtl/create_dataset_noise.py exp_NOISE{NOISE}/1.yaml\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from mtl import data_setup, models, engine, utils\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Config\n",
    "parser = argparse.ArgumentParser(description='specifies config file')\n",
    "parser.add_argument('yaml_file', metavar='yaml_file', type=str, help='specify config file name with .yaml extension')\n",
    "args = parser.parse_args()\n",
    "yaml_file = args.yaml_file\n",
    "CONFIG_PATH = os.getcwd() + '/config'\n",
    "config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "N=config['N']\n",
    "DIM=config['DIM']\n",
    "N_AUX = config['N_AUX']\n",
    "COMPLEXITY = len(config['HIDDENS'])\n",
    "experiment_name = config['experiment_name']\n",
    "\n",
    "if N_AUX != 1:\n",
    "    raise Exception(\"N_AUX should be 1\")\n",
    "\n",
    "# Save path\n",
    "# timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "\n",
    "# data_path = os.getcwd() + '/data/'\n",
    "# sub_path = f'DIM={DIM}/N_AUX={N_AUX}'#/C={COMPLEXITY}'\n",
    "# save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "# save_path.mkdir(parents=True,\n",
    "#                 exist_ok=True)\n",
    "# save_path = str(save_path)\n",
    "# print(save_path)\n",
    "\n",
    "# Actions\n",
    "# teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "# torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "train_data, test_data, val_data, save_path = data_setup.make_data_noise(yaml_file=yaml_file)\n",
    "\n",
    "torch.save(train_data, save_path + '/train_data.pt')\n",
    "torch.save(test_data, save_path + '/test_data.pt')\n",
    "torch.save(val_data, save_path + '/val_data.pt')\n",
    "\n",
    "print(f'Datafiles saved to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_dataset_noisy_labels.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/create_dataset_noisy_labels.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/create_dataset_noisy_labels.py\n",
    "\n",
    "'''\n",
    "# Command line arguments --> only config file (https://www.youtube.com/watch?v=FbEJN8FsJ9U)\n",
    "\n",
    "Usage:\n",
    "    import numpy as np\n",
    "    noise_list = ['0.01', '0.05', '0.10', '0.15','0.20', '0.30', '0.40', '0.50', '0.60', '0.70', '0.80', '0.90', '1.00', '1.10']\n",
    "    for NOISE in noise_list:\n",
    "        !python mtl/create_dataset_noisy_labels.py exp_NOISY_LABEL{NOISE}/1.yaml\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from mtl import data_setup, models, engine, utils\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Config\n",
    "parser = argparse.ArgumentParser(description='specifies config file')\n",
    "parser.add_argument('yaml_file', metavar='yaml_file', type=str, help='specify config file name with .yaml extension')\n",
    "args = parser.parse_args()\n",
    "yaml_file = args.yaml_file\n",
    "CONFIG_PATH = os.getcwd() + '/config'\n",
    "config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "N=config['N']\n",
    "DIM=config['DIM']\n",
    "N_AUX = config['N_AUX']\n",
    "COMPLEXITY = len(config['HIDDENS'])\n",
    "experiment_name = config['experiment_name']\n",
    "\n",
    "if N_AUX != 1:\n",
    "    raise Exception(\"N_AUX should be 1\")\n",
    "\n",
    "# Save path\n",
    "# timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "\n",
    "# data_path = os.getcwd() + '/data/'\n",
    "# sub_path = f'DIM={DIM}/N_AUX={N_AUX}'#/C={COMPLEXITY}'\n",
    "# save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "# save_path.mkdir(parents=True,\n",
    "#                 exist_ok=True)\n",
    "# save_path = str(save_path)\n",
    "# print(save_path)\n",
    "\n",
    "# Actions\n",
    "# teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "# torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "train_data, test_data, val_data, save_path = data_setup.make_data_noisy_labels(yaml_file=yaml_file)\n",
    "\n",
    "torch.save(train_data, save_path + '/train_data.pt')\n",
    "torch.save(test_data, save_path + '/test_data.pt')\n",
    "torch.save(val_data, save_path + '/val_data.pt')\n",
    "\n",
    "print(f'Datafiles saved to {save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_dataset_noisy_antagonist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/create_dataset_noisy_antagonist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/create_dataset_noisy_antagonist.py\n",
    "\n",
    "'''\n",
    "# Command line arguments --> only config file (https://www.youtube.com/watch?v=FbEJN8FsJ9U)\n",
    "\n",
    "Usage:\n",
    "    for i in range(10):\n",
    "        !python mtl/create_dataset_noisy_antagonist.py exp_antagonist{i}/1.yaml\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from mtl import data_setup, models, engine, utils\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Config\n",
    "parser = argparse.ArgumentParser(description='specifies config file')\n",
    "parser.add_argument('yaml_file', metavar='yaml_file', type=str, help='specify config file name with .yaml extension')\n",
    "args = parser.parse_args()\n",
    "yaml_file = args.yaml_file\n",
    "CONFIG_PATH = os.getcwd() + '/config'\n",
    "config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "N=config['N']\n",
    "DIM=config['DIM']\n",
    "N_AUX = config['N_AUX']\n",
    "COMPLEXITY = len(config['HIDDENS'])\n",
    "experiment_name = config['experiment_name']\n",
    "ANTAGONIST = config['ANTAGONIST']\n",
    "\n",
    "if N_AUX < ANTAGONIST:\n",
    "    raise Exception(\"N_AUX < ANTAGONIST\")\n",
    "\n",
    "# Save path\n",
    "# timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "\n",
    "# data_path = os.getcwd() + '/data/'\n",
    "# sub_path = f'DIM={DIM}/N_AUX={N_AUX}/ANTAGONIST={ANTAGONIST}'\n",
    "# save_path = Path(os.path.join(data_path, experiment_name, sub_path))\n",
    "# save_path.mkdir(parents=True,\n",
    "#                 exist_ok=True)\n",
    "# save_path = str(save_path)\n",
    "# print(save_path)\n",
    "\n",
    "# Actions\n",
    "# teacher = models.Teacher(yaml_file=yaml_file).to(device)\n",
    "# torch.save(obj=teacher.state_dict(), f=save_path+'/teacher.pth')\n",
    "\n",
    "train_data, test_data, val_data, save_path = data_setup.make_data_noisy_antagonist(yaml_file=yaml_file)\n",
    "\n",
    "torch.save(train_data, save_path + '/train_data.pt')\n",
    "torch.save(test_data, save_path + '/test_data.pt')\n",
    "torch.save(val_data, save_path + '/val_data.pt')\n",
    "\n",
    "print(f'Datafiles saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/models.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import yaml\n",
    "import argparse \n",
    "import os\n",
    "try:\n",
    "    import utils\n",
    "except: from mtl_modular import utils\n",
    "\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self, yaml_file):\n",
    "        super().__init__()\n",
    "        \n",
    "        CONFIG_PATH = os.getcwd() + '/config/'\n",
    "        config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "        \n",
    "        self.hidden_sizes = config['HIDDENS']\n",
    "        self.n_aux = config['N_AUX']\n",
    "        self.dim = config['DIM']\n",
    "        self.init = config['INIT']\n",
    "        self.sigma = config['INIT_VALUE']\n",
    "\n",
    "        act = config['ACTIVATION']\n",
    "        if act == 'SIGMOID':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif act == 'TANH':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif act == 'RELU':\n",
    "            self.activation = nn.ReLU()\n",
    "        else: \n",
    "            raise Exception('[ERROR] Activation must be one of SIGMOID, TANH, RELU')\n",
    "\n",
    "        # Layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.aux_layers = nn.ModuleList()\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = nn.Linear(in_features=self.dim, out_features=self.hidden_sizes[0])\n",
    "        self.hidden_layers.append(nn.LayerNorm(self.hidden_sizes[0]))\n",
    "        self.hidden_layers.append(self.activation)\n",
    "\n",
    "        # Hidden Layers\n",
    "        for i in range(len(self.hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(self.hidden_sizes[i], self.hidden_sizes[i+1]))\n",
    "            self.hidden_layers.append(nn.LayerNorm(self.hidden_sizes[i+1]))\n",
    "            self.hidden_layers.append(self.activation)\n",
    "\n",
    "        # Main Output Layer\n",
    "        self.main_output_layer = nn.Linear(in_features=self.hidden_sizes[-1], out_features=1)\n",
    "        \n",
    "        # Auxiliary Output Layers\n",
    "        for n in range(self.n_aux):\n",
    "            self.aux_layers.append(nn.Linear(in_features=self.hidden_sizes[-1], out_features=1))        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if self.init == 'UNIFORM':\n",
    "                module.weight.data.uniform_(-self.sigma, +self.sigma)\n",
    "            elif self.init == 'GAUSSIAN':\n",
    "                module.weight.data.normal_(mean=0., std=self.sigma)\n",
    "            else:\n",
    "                raise Exception('[ERROR] INIT must be one of UNIFORM, GAUSSIAN.')\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                if self.init == 'UNIFORM':\n",
    "                    module.bias.data.uniform_(-self.sigma, +self.sigma)\n",
    "                elif self.init == 'GAUSSIAN':\n",
    "                    module.bias.data.normal_(mean=0., std=self.sigma)\n",
    "                else:\n",
    "                    raise Exception('[ERROR] INIT must be one of UNIFORM, GAUSSIAN.')\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        \n",
    "        # Main task\n",
    "        y_main = self.main_output_layer(z)\n",
    "        \n",
    "        #Auxiliary tasks\n",
    "        lista = []\n",
    "        for layer in self.aux_layers:\n",
    "            lista.append(layer(z))\n",
    "        y_alpha = torch.cat(lista, dim=1)\n",
    "        \n",
    "        out = torch.cat([y_main, y_alpha], dim=1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class Student(nn.Module):\n",
    "    def __init__(self, yaml_file):\n",
    "        \n",
    "        CONFIG_PATH = os.getcwd() + '/config/'\n",
    "        config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_sizes = config['S_HIDDENS']\n",
    "        self.n_aux = config['N_AUX']\n",
    "        self.dim = config['DIM']\n",
    "        act = config['ACTIVATION']\n",
    "\n",
    "        self.init = config['INIT']\n",
    "        self.sigma = config['INIT_VALUE']\n",
    "        self.norm = config['LAYER_NORM']\n",
    "\n",
    "    \n",
    "        if act == 'SIGMOID':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif act == 'TANH':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif act == 'RELU':\n",
    "            self.activation = nn.ReLU()\n",
    "        else: \n",
    "            print('[WARNING] Activation must be one of SIGMOID, TANH, RELU')\n",
    "            pass\n",
    "        \n",
    "        # Layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.aux_layers = nn.ModuleList()\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = nn.Linear(in_features=self.dim, out_features=self.hidden_sizes[0])\n",
    "        if self.norm:\n",
    "            self.hidden_layers.append(nn.LayerNorm(self.hidden_sizes[0]))\n",
    "        self.hidden_layers.append(self.activation)\n",
    "        \n",
    "        # Hidden Layers\n",
    "        for i in range(len(self.hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(self.hidden_sizes[i], self.hidden_sizes[i+1]))\n",
    "            if self.norm:\n",
    "                self.hidden_layers.append(nn.LayerNorm(self.hidden_sizes[i+1]))\n",
    "            self.hidden_layers.append(self.activation)\n",
    "\n",
    "        # Main Output Layer\n",
    "        self.main_output_layer = nn.Linear(in_features=self.hidden_sizes[-1], out_features=1)\n",
    "        \n",
    "        # Auxiliary Output Layers\n",
    "        for n in range(self.n_aux):\n",
    "            self.aux_layers.append(nn.Linear(in_features=self.hidden_sizes[-1], out_features=1))        \n",
    "        \n",
    "        self.apply(self._init_weights)    \n",
    "    \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if self.init == 'UNIFORM':\n",
    "                module.weight.data.uniform_(-self.sigma, +self.sigma)\n",
    "            elif self.init == 'GAUSSIAN':\n",
    "                module.weight.data.normal_(mean=0., std=self.sigma)\n",
    "            else:\n",
    "                raise Exception('[ERROR] INIT must be one of UNIFORM, GAUSSIAN.')\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                if self.init == 'UNIFORM':\n",
    "                    module.bias.data.uniform_(-self.sigma, +self.sigma)\n",
    "                elif self.init == 'GAUSSIAN':\n",
    "                    module.bias.data.normal_(mean=0., std=self.sigma)\n",
    "                else:\n",
    "                    raise Exception('[ERROR] INIT must be one of UNIFORM, GAUSSIAN.')\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.input_layer(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        \n",
    "        # Main task\n",
    "        y_main = self.main_output_layer(z)\n",
    "        \n",
    "        #Auxiliary tasks\n",
    "        lista = []\n",
    "        for layer in self.aux_layers:\n",
    "            lista.append(layer(z))    \n",
    "        y_alpha = torch.cat(lista, dim=1)\n",
    "        \n",
    "        out = torch.cat([y_main, y_alpha], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mtl/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/engine.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Dict, List, Tuple\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "def train_step(model: nn.Module, \n",
    "                dataloader: torch.utils.data.DataLoader,\n",
    "                loss_fn: nn.Module,\n",
    "                loss_aux: nn.Module,\n",
    "                optimizer: torch.optim.Optimizer, \n",
    "                device: torch.device, \n",
    "                BETAS: torch.tensor)-> Tuple[float, float]:\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    BETAS.to(device)\n",
    "    # print(f'BETAS: {BETAS}')\n",
    "\n",
    "    loss_tracker = torch.zeros(1, len(BETAS))\n",
    "\n",
    "    for batch, (X_,y_) in enumerate(dataloader):\n",
    "        X_, y_ = X_.to(device), y_.to(device)\n",
    "        preds = model(X_).to(device)\n",
    "\n",
    "        lista = []\n",
    "        for i in range(y_.shape[1]):\n",
    "            lista.append(loss_aux(preds[:, i], y_[:, i]))\n",
    "        \n",
    "        # Tensor with each task loss as entry, first entry is MAIN TASK\n",
    "        losses = torch.hstack(lista).to(device) \n",
    "        BETAS[0] = 1. # Main loss has fixed BETA\n",
    "        \n",
    "        # Weighted sum\n",
    "        loss = torch.dot(losses, BETAS)\n",
    "\n",
    "        # Loss tracking\n",
    "        for i in range(loss_tracker.shape[1]):\n",
    "            loss_tracker[:, i] += losses[i].item()\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_tracker = loss_tracker / len(dataloader)\n",
    "    \n",
    "    return loss_tracker\n",
    "\n",
    "    \n",
    "def test_step(model: nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: nn.Module,\n",
    "              device=torch.device) -> float:\n",
    "    '''\n",
    "    Only evaluate on main task\n",
    "    '''\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    n_tasks = next(iter(dataloader))[1][0].shape[0]\n",
    "    test_loss_tracker = torch.zeros(1, n_tasks)\n",
    "\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        # Loop through batches\n",
    "        for batch, (X_,y_) in enumerate(dataloader):\n",
    "            # Data to device\n",
    "            X_, y_ = X_.to(device), y_.to(device)\n",
    "            \n",
    "            preds = model(X_).to(device)\n",
    "\n",
    "            lista = []\n",
    "            for i in range(y_.shape[1]):\n",
    "                lista.append(loss_fn(preds[:, i], y_[:, i]))\n",
    "\n",
    "            losses = torch.hstack(lista).to(device) \n",
    "\n",
    "            # Loss tracking\n",
    "            for i in range(test_loss_tracker.shape[1]):\n",
    "                test_loss_tracker[:, i] += losses[i].item()\n",
    "\n",
    "    test_loss_tracker = test_loss_tracker / len(dataloader)\n",
    "\n",
    "    return test_loss_tracker\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          loss_fn: torch.nn.Module,\n",
    "          loss_aux: torch.nn.Module, \n",
    "          epochs: int,\n",
    "          BETAS: torch.tensor,\n",
    "          patience: int,\n",
    "          device: torch.device)  -> Dict[str, List[float]]:\n",
    "    \n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    - model: A PyTorch model to be trained and tested.\n",
    "    - train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    - test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    - optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    - loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    - loss_aux: A PyTorch loss function for auxiliary tasks --> da rendere modificabile??\n",
    "    - epochs: An integer indicating how many epochs to train for.\n",
    "    - BETAS: Tensor containing beta values for different tasks.\n",
    "    - device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "    - writer: SummaryWriter instance to track result in tensorboard.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss_main: [...],\n",
    "              train_loss_tot: [...],\n",
    "              train_loss_aux: [...],\n",
    "              test_loss_main: [...]} \n",
    "    \"\"\"\n",
    "    \n",
    "    results = {f'{key}{i}': []  for i in range(len(BETAS)) for key in ['train', 'test']}\n",
    "\n",
    "    # Early Stopping       \n",
    "    best_score = None\n",
    "    #best_score_aux = None\n",
    "    counter = 0\n",
    "    patience = patience\n",
    "        \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Initialization check\n",
    "        if epoch==1:\n",
    "            test_losses = test_step(model=model, \n",
    "                                            dataloader=test_dataloader, \n",
    "                                            loss_fn=loss_fn,\n",
    "                                            device=device)                               \n",
    "            print(f'test_loss_main: {test_losses[0, 0].item()} | test_loss_aux: {test_losses[0, 1].item()}')\n",
    "\n",
    "        # Steps\n",
    "        train_losses = train_step(model=model, \n",
    "                                               dataloader=train_dataloader, \n",
    "                                               loss_fn=loss_fn,\n",
    "                                               loss_aux=loss_aux,\n",
    "                                               optimizer=optimizer,\n",
    "                                               device=device, \n",
    "                                                BETAS=BETAS)\n",
    "        test_losses = test_step(model=model, \n",
    "                                        dataloader=test_dataloader, \n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "        # Printing message \n",
    "        if (epoch+1)%20 == 0:\n",
    "            print(\n",
    "              f\"Epoch: {epoch+1} | \"\n",
    "              f\"Betas: {[round(item, 2) for item in BETAS.tolist()]} | \"\n",
    "              f\"train_loss_main: {train_losses[0, 0]:.4f} | \"\n",
    "              f\"test_loss_main: {test_losses[0, 0]:.4f} | \" \n",
    "              )\n",
    "                \n",
    "        # Fill return object\n",
    "        for i in range(len(BETAS)):\n",
    "            results[f'train{i}'].append(train_losses[0, i].item())\n",
    "            results[f'test{i}'].append(test_losses[0, i].item())\n",
    "\n",
    "        # Early Stopping\n",
    "        test_loss_main = test_losses[0, 0].item()\n",
    "        if best_score is None:\n",
    "            best_score = test_loss_main\n",
    "            #best_score_aux = test_loss_aux\n",
    "        else:\n",
    "            # MAIN LOSS\n",
    "            if test_loss_main < best_score:\n",
    "                # val_loss improves, we update the latest best_score, \n",
    "                best_score = test_loss_main\n",
    "            else:\n",
    "                # val_loss does not improve, we increase the counter, \n",
    "                # stop training if it exceeds the amount of patience\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f'[EARLY STOPPING] Epoch: {epoch} | Test Loss Main: {best_score}\\n'\n",
    "                    '-----------------------------------------------------------------------------')\n",
    "                    break    \n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Debugging Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mtl import data_setup\n",
    "\n",
    "# data_path = '/mnt/c/Users/Cesare/Desktop/Tesi/code/data/DIM=20/N_AUX=3/C=1'\n",
    "\n",
    "# train_data = torch.load(data_path + '/train_data.pt')\n",
    "# test_data = torch.load(data_path + '/test_data.pt')\n",
    "\n",
    "# X_train = train_data.tensors[0]\n",
    "# y_train = train_data.tensors[1]\n",
    "\n",
    "# X_test = test_data.tensors[0]\n",
    "# y_test = test_data.tensors[1]\n",
    "\n",
    "# # Create a new TensorDataset with the first N elements\n",
    "# train_data = TensorDataset(X_train[:100], y_train[:100])\n",
    "# test_data  = TensorDataset(X_test[:2000], y_test[:2000])\n",
    "\n",
    "\n",
    "# train_dataloader, test_dataloader = data_setup.make_dataloaders(\n",
    "#                                                         train_data = train_data, \n",
    "#                                                         test_data = test_data,\n",
    "#                                                         batch_size=32,\n",
    "#                                                         num_workers=4)\n",
    "# train_dataloader\n",
    "# n_tasks = next(iter(train_dataloader))[1][0].shape[0]\n",
    "# n_tasks\n",
    "# losses = torch.zeros(1, n_tasks)\n",
    "# losses\n",
    "\n",
    "# aa = torch.ones(1, n_tasks)\n",
    "\n",
    "# j = torch.cat([losses, aa], dim=0)\n",
    "# j\n",
    "# loss_tracker = torch.zeros(1, n_tasks)\n",
    "# loss_tracker.shape\n",
    "\n",
    "# for i in range(loss_tracker.shape[1]):\n",
    "#     loss_tracker[:, i] = i\n",
    "\n",
    "# loss_tracker[0, 3].item()\n",
    "# betasss  = torch.zeros(1, 3)\n",
    "# betasss\n",
    "# betasss.tolist()\n",
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mtl/train0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl/train0.py\n",
    "\n",
    "'''\n",
    "Usage : !python mtl_modular/train.py exp_1/my_config.yaml\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "# In script\n",
    "try:\n",
    "    import data_setup, engine, models, utils\n",
    "# In jupyter\n",
    "except:\n",
    "    from mtl import data_setup, engine, models, utils\n",
    "\n",
    "\n",
    "# Command line arguments --> only config file (https://www.youtube.com/watch?v=FbEJN8FsJ9U)\n",
    "parser = argparse.ArgumentParser(description='specifies config file')\n",
    "parser.add_argument('yaml_file', metavar='yaml_file', type=str, help='specify config file name with .yaml extension')\n",
    "args = parser.parse_args()\n",
    "yaml_file = args.yaml_file\n",
    "\n",
    "# folder to load config file\n",
    "CONFIG_PATH = os.getcwd() + '/config'\n",
    "config = utils.load_config(path=CONFIG_PATH, config_name=yaml_file)\n",
    "\n",
    "# DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# DIRECTORIES\n",
    "# history_path = config['history_path'] + config['experiment_name']\n",
    "# saved_models_path = config['saved_models_path'] + config['experiment_name']\n",
    "# images_path = config['images_path'] + config['experiment_name']\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%m-%d-%Y\") \n",
    "\n",
    "# EXPERIMENT\n",
    "experiment_name = config['experiment_name']\n",
    "\n",
    "# DATA\n",
    "all_data_path = os.getcwd() + '/data/'\n",
    "\n",
    "N=config['N']\n",
    "DIM=config['DIM']\n",
    "N_AUX = config['N_AUX']\n",
    "NOISE = config['NOISE']\n",
    "print(f'NOISE = {NOISE}')\n",
    "\n",
    "sub_path = f'DIM={DIM}/N_AUX={N_AUX}'\n",
    "data_path = os.path.join(all_data_path, experiment_name, sub_path)\n",
    "\n",
    "if 'nois' in experiment_name:\n",
    "    data_path = data_path + f'/NOISE={NOISE}'\n",
    "\n",
    "if 'antagonist' in experiment_name:\n",
    "    data_path = data_path + f'/ANTAGONIST={ANTAGONIST}'\n",
    "\n",
    "# antagonist and noise\n",
    "if 'NOISE' in experiment_name:\n",
    "    data_path = data_path + f'/ANTAGONIST={ANTAGONIST}'\n",
    "\n",
    "# New runs to show MTL does not work\n",
    "if 'different_teachers' in experiment_name:\n",
    "    data_path = os.getcwd() + '/data/unrelated/different_teachers'\n",
    "\n",
    "if 'randomized' in experiment_name:\n",
    "    data_path = os.getcwd() + '/data/unrelated/randomized'\n",
    "\n",
    "print(f'Data file from : \\n {data_path}')\n",
    "\n",
    "try: \n",
    "    NOISE = float(config['NOISE'])\n",
    "except: pass\n",
    "try:\n",
    "    ANTAGONIST = config['ANTAGONIST']\n",
    "except: pass\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "RANDOM_SEED = config['RANDOM_SEED']\n",
    "SAVE_MODE = config['SAVE_MODE']\n",
    "ATTEMPTS = config['ATTEMPTS']\n",
    "EPOCHS = config['EPOCHS']\n",
    "BATCH_SIZE = config['BATCH_SIZE']\n",
    "PATIENCE = config['PATIENCE']\n",
    "OPTIMIZER = config['OPTIMIZER']\n",
    "LR = float(config['LR'])\n",
    "\n",
    "# Betas\n",
    "BETA_START = config['BETA_START']\n",
    "# MAX_BETA = config['MAX_BETA']\n",
    "# dBETA = (MAX_BETA - BETA_START) / ATTEMPTS\n",
    "\n",
    "# Beta tutti uguali\n",
    "#BETAS = torch.zeros(N_AUX+1)\n",
    "BETAS = torch.full(size=[N_AUX+1], fill_value=BETA_START).to(device)\n",
    "\n",
    "# Beta random\n",
    "#BETAS = torch.rand(N_AUX+1)\n",
    "\n",
    "# start_time = timer()\n",
    "\n",
    "# Load tensors and Create DataLoaders\n",
    "\n",
    "train_data = torch.load(data_path + '/train_data.pt')\n",
    "test_data = torch.load(data_path + '/test_data.pt')\n",
    "\n",
    "X_train = train_data.tensors[0]\n",
    "y_train = train_data.tensors[1]\n",
    "\n",
    "X_test = test_data.tensors[0]\n",
    "y_test = test_data.tensors[1]\n",
    "\n",
    "# Create a new TensorDataset with the first N elements\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "dataset_size = N\n",
    "indices = torch.randperm(dataset_size)\n",
    "train_data = TensorDataset(X_train[indices], y_train[indices])\n",
    "\n",
    "test_data  = TensorDataset(X_test[:2000], y_test[:2000])\n",
    "\n",
    "train_dataloader, test_dataloader = data_setup.make_dataloaders(\n",
    "                                                        train_data = train_data, \n",
    "                                                        test_data = test_data,\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        num_workers=4)\n",
    "\n",
    "\n",
    "## ---------------- BETAS -------------------------------\n",
    "# BETA_VALUES = np.logspace(-1, 1, num=21, endpoint=True) # Log space for [0, 10] interval\n",
    "# BETA_VALUES = np.concatenate((np.zeros(1), BETA_VALUES), axis=0)\n",
    "\n",
    "# BETA_VALUES = np.linspace(0, 1., num=11, endpoint=True) # Lin space for [0, 1.5] internal\n",
    "\n",
    "BETA_VALUES = np.linspace(0, 1., num=11, endpoint=True)\n",
    "bb = np.linspace(2., 10., num=9, endpoint=True)\n",
    "BETA_VALUES = np.hstack([BETA_VALUES, bb])\n",
    "\n",
    "# bb = np.linspace(2., 10., num=9, endpoint=True)\n",
    "# BETA_VALUES = bb\n",
    "\n",
    "## ------------------------------------------------------\n",
    "\n",
    "for beta in BETA_VALUES:\n",
    "\n",
    "    BETAS[1:] = beta\n",
    "    try: del student\n",
    "    except: pass\n",
    "    \n",
    "    # torch.manual_seed(RANDOM_SEED)\n",
    "    # torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    \n",
    "    student = models.Student(yaml_file=yaml_file).to(device)\n",
    "    \n",
    "    loss_main = nn.MSELoss()\n",
    "    loss_aux = nn.MSELoss()\n",
    "\n",
    "    if OPTIMIZER=='SGD':\n",
    "        optimizer = torch.optim.SGD(params=student.parameters(), lr=LR)    \n",
    "    elif OPTIMIZER=='ADAM':\n",
    "        optimizer = torch.optim.Adam(params=student.parameters(), lr=LR, weight_decay=1e-4) # weight decay default\n",
    "    else:\n",
    "        raise Exception('[ERROR] Optimizer must be one of SGD, ADAM')\n",
    "\n",
    "\n",
    "    history = []\n",
    "    history = engine.train(model=student, \n",
    "                    train_dataloader=train_dataloader, \n",
    "                    test_dataloader=test_dataloader,\n",
    "                    optimizer=optimizer, \n",
    "                    loss_fn=loss_main,\n",
    "                    loss_aux=loss_aux, \n",
    "                    epochs= EPOCHS, \n",
    "                    BETAS=BETAS,\n",
    "                    patience=PATIENCE,\n",
    "                    device=device)\n",
    "        \n",
    "    if SAVE_MODE:\n",
    "        save_path = utils.path_handler(CONFIG_PATH = os.getcwd() + '/config',  yaml_file=yaml_file)\n",
    "        \n",
    "        utils.history_to_csv(history=history, \n",
    "                            target_dir_path=save_path,\n",
    "                            model_name=f'Beta={BETAS[1]:.2f}')\n",
    "        \n",
    "    # Saving model\n",
    "        utils.save_model(model=student,\n",
    "                        target_dir_path=save_path,\n",
    "                        model_name=f'Beta={BETAS[1]:.2f}.pth')\n",
    "        print('-----------------------------------------------------------------------------')\n",
    "    else: pass\n",
    "    \n",
    "    \n",
    "#    if BETAS[1] < 0.4:\n",
    "#        BETAS[1:] += 0.1\n",
    "#    elif 0.4 <= BETAS[1] <= 2.5:\n",
    "#        BETAS[1:] += dBETA\n",
    "#    else:\n",
    "#        BETAS[1:] += 0.4\n",
    "\n",
    "    # if BETAS[1] < 1.0:\n",
    "    #     BETAS[1:] += 0.05\n",
    "\n",
    "    # elif BETAS[1] < 2. :\n",
    "    #     BETAS[1:] += 0.2\n",
    "\n",
    "    # else: \n",
    "    #     BETAS[1:] += 2.\n",
    "\n",
    "\n",
    "#    print(f'type Betas : {type(BETAS)}')\n",
    "#    print(f'shape Betas : {BETAS.shape}')\n",
    "#    print(f'Betas[0] : {BETAS[0]}')\n",
    "#    print(f'Betas[1] : {BETAS[1]}')\n",
    "\n",
    "        \n",
    "utils.copy_file(yaml_file=yaml_file,\n",
    "                target_dir_path=save_path,\n",
    "                source=os.getcwd() + '/config/' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mtl_modular/config_writer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mtl_modular/config_writer.py\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def write_yaml_to_file(data, filename, experiment):\n",
    "    directory = f'config/{experiment}/'\n",
    "    directory = Path(directory)\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(f'{directory}/{filename}.yaml', 'w',) as f :\n",
    "        yaml.dump(data, f, sort_keys=False) \n",
    "    #print('Written to file successfully')\n",
    "\n",
    "data = {\n",
    "'data_path': '../data',\n",
    "'history_path': '../history',\n",
    "'saved_models_path': '../saved_models',\n",
    "'images_path': '../images',\n",
    "'experiment_name': 'exp_aux1',\n",
    "'subexperiment_name' : 'sub_1',   \n",
    "'HIDDENS': list([10]),\n",
    "'S_HIDDENS': list([10]), \n",
    "\n",
    "'N': 1000,\n",
    "'DIM': 20,\n",
    "'RANDOM_SEED': 31,\n",
    "'SAVE_MODE': True,\n",
    "'ATTEMPTS': 30,\n",
    "'EPOCHS': 500, \n",
    "'BATCH_SIZE': 32,\n",
    "'PATIENCE' : 10, \n",
    "'N_AUX': 1,\n",
    "'LR': '1e-2',\n",
    "'OPTIMIZER' : 'ADAM',\n",
    "'ACTIVATION': 'TANH',\n",
    "'BETA_START': 0., \n",
    "'MAX_BETA': 11.\n",
    "}\n",
    "counter = int(1)\n",
    "experiment = data['experiment_name']\n",
    "subexperiment = data['subexperiment_name']\n",
    "\n",
    "# ---------- single for debug ------------------------------\n",
    "# write_yaml_to_file(data, f'{counter}', experiment)\n",
    "\n",
    "# ---------- real exp --------------------------------------\n",
    "# ---------- Varia N--------------------------------------\n",
    "\n",
    "# for j in range(0, 2):\n",
    "# RANGE_N = [100, 200, 500, 800, 1000, 1400, 1700, 2000]\n",
    "# for N in RANGE_N:\n",
    "#     counter = int(1)\n",
    "#     data['N'] = N\n",
    "#     # data['N'] += 500\n",
    "#     # N = data['N']\n",
    "#     data['experiment_name'] = f'exp_{N}'\n",
    "\n",
    "#     for i in range(1, 20): \n",
    "#         data['subexperiment_name'] = f'sub_{i}'\n",
    "#         experiment = data['experiment_name']\n",
    "#         write_yaml_to_file(data, f'{counter}', experiment)\n",
    "#         data['RANDOM_SEED'] += 5\n",
    "#         counter+=1\n",
    "    \n",
    "for n_aux in range(5, 10, 1):\n",
    "    counter = int(1)\n",
    "    data['N_AUX'] = n_aux\n",
    "    data['experiment_name'] = f'exp_aux{n_aux}'\n",
    "    \n",
    "    for i in range(1, 20): \n",
    "        data['subexperiment_name'] = f'sub_{i}'\n",
    "        experiment = data['experiment_name']\n",
    "        write_yaml_to_file(data, f'{counter}', experiment)\n",
    "        data['RANDOM_SEED'] += 5\n",
    "        counter+=1\n",
    "    \n",
    "print(f'Successfully written to config/{experiment}, {counter} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully written to config/exp_aux9, 20 files\n"
     ]
    }
   ],
   "source": [
    "!python mtl_modular/config_writer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=1/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=2/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=3/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=4/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=5/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=6/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=7/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=8/C=1\n",
      "Datafiles saved to /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=9/C=1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    !python mtl/create_dataset.py exp_aux{i}/1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    for j in range(1, 20):\n",
    "        !python mtl/train.py exp_aux{i}/{j}.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file from : \n",
      " /home/cesare/TESI/Tesi/code/data/DIM=20/N_AUX=3/C=1\n",
      "Epoch: 20 | Betas: [1.0, 0.0, 0.0, 0.0] | train_loss_main: 0.0137 | test_loss_main: 0.0165 | \n",
      "Epoch: 40 | Betas: [1.0, 0.0, 0.0, 0.0] | train_loss_main: 0.0070 | test_loss_main: 0.0105 | \n",
      "[EARLY STOPPING] Epoch: 42 | Test Loss Main: 0.009619008749723434\n",
      "-----------------------------------------------------------------------------\n",
      "[INFO] Created .csv file to /home/cesare/TESI/Tesi/code/experiments/04-20-2024/exp_aux3/sub_1/N=1000/DIM=20/N_AUX=3/EPOCHS=500/Beta=0.00.csv\n",
      "[INFO] Saving model to: /home/cesare/TESI/Tesi/code/experiments/04-20-2024/exp_aux3/sub_1/N=1000/DIM=20/N_AUX=3/EPOCHS=500/Beta=0.00.pth \n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "Epoch: 20 | Betas: [1.0, 0.1, 0.1, 0.1] | train_loss_main: 0.0097 | test_loss_main: 0.0135 | \n",
      "[EARLY STOPPING] Epoch: 36 | Test Loss Main: 0.006920445244759321\n",
      "-----------------------------------------------------------------------------\n",
      "[INFO] Created .csv file to /home/cesare/TESI/Tesi/code/experiments/04-20-2024/exp_aux3/sub_1/N=1000/DIM=20/N_AUX=3/EPOCHS=500/Beta=0.10.csv\n",
      "[INFO] Saving model to: /home/cesare/TESI/Tesi/code/experiments/04-20-2024/exp_aux3/sub_1/N=1000/DIM=20/N_AUX=3/EPOCHS=500/Beta=0.10.pth \n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cesare/TESI/Tesi/code/mtl/train.py\", line 142, in <module>\n",
      "    history = engine.train(model=student, \n",
      "  File \"/home/cesare/TESI/Tesi/code/mtl/engine.py\", line 146, in train\n",
      "    train_losses = train_step(model=model, \n",
      "  File \"/home/cesare/TESI/Tesi/code/mtl/engine.py\", line 43, in train_step\n",
      "    loss.backward(retain_graph=True)\n",
      "  File \"/home/cesare/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/cesare/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python mtl/train.py exp_aux3/1.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
